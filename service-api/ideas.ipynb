{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ручка 3. Summary\n",
    "\n",
    "Делает краткое саммари всего конспекта (на вход идет весь файл в обсидиане, можно добавить передачу всей папки)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.gigachat import GigaChat\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GIGACHAT_CREDENTIALS = os.environ.get(\"GIGACHAT_CREDENTIALS\")\n",
    "SERPER_API_KEY = os.environ.get(\"SERPER_API_KEY\")\n",
    "\n",
    "llm = GigaChat(\n",
    "    credentials=GIGACHAT_CREDENTIALS,\n",
    "    model=\"GigaChat-Pro\",\n",
    "    verify_ssl_certs=True,\n",
    "    scope=\"GIGACHAT_API_CORP\",\n",
    "    profanity_check=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем конспект\n",
    "from langchain_community.document_loaders.text import TextLoader\n",
    "\n",
    "notes_path = \"./notes/small_note.md\"\n",
    "\n",
    "loader = TextLoader(notes_path, \"utf-8\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Используем Stuff Documents Chain + Summarization Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nonam\\Documents\\python_local\\obsidian-gigachat\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Влияние параметров пара на показатели комбинированной установки с простым паровым контуром\n",
      "- Максимальная мощность паровой турбины достигается при максимальных параметрах пара\n",
      "- Мощность паровой турбины зависит от расхода пара, располагаемого перепада энтальпий на турбину и КПД паровой турбины\n",
      "- Относительный расход пара определяется уравнением теплового баланса испарительной и перегревательной поверхностей котла-утилизатора\n",
      "- Температура пара перед паровой турбиной принимается максимально возможной для уменьшения потерь от влажности и повышения КПД турбины\n",
      "- С увеличением давления пара относительный расход пара снижается из-за снижения удельной теплоты парообразования\n",
      "- С ростом давления растет температура уходящих газов и располагаемый перепад энтальпий на паровую турбину\n",
      "- Совместное влияние параметров на величину мощности паровой турбины приводит к слабой зависимости КПД парогазовой установки от начального давления\n",
      "- Температура уходящих газов остается выше минимально допустимой, что позволяет более полно использовать теплоту уходящих газов в контуре низкого давления\n",
      "- Влияние минимального температурного напора аналогично влиянию температуры газа перед котлом-утилизатором\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMSummarizationCheckerChain\n",
    "\n",
    "def stuff_summary(docs, checker: bool=False, max_checks:int=2):\n",
    "     prompt_template = \"\"\"Выдели клюбючевые моменты в тексте.\n",
    "     Оформи ответ в виде списка.\n",
    "     Текст: \"{context}\"\n",
    "     Ключевые моменты:\"\"\"\n",
    "     prompt = PromptTemplate.from_template(prompt_template)\n",
    "     stuff_chain = create_stuff_documents_chain(llm, prompt)\n",
    "     res = stuff_chain.invoke({\"context\": docs})\n",
    "     if not checker:\n",
    "          return res\n",
    "     else:\n",
    "          checker_chain = LLMSummarizationCheckerChain.from_llm(llm, max_checks=max_checks)\n",
    "          return checker_chain.invoke(res)\n",
    "          \n",
    "\n",
    "print(stuff_summary(docs, True, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Используем Map-Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Основные темы, представленные в документах, включают влияние параметров пара на показатели комбинированной установки с простым паровым контуром, зависимость мощности паровой турбины от расхода пара, располагаемого перепада энтальпий на турбину и КПД паровой турбины, определение относительного расхода пара через уравнение теплового баланса испарительной и перегревательной поверхностей котла-утилизатора, влияние давления пара на относительный расход пара и температуру уходящих газов, а также влияние температуры пара перед паровой турбиной на влажность и КПД турбины.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "\n",
    "# Map\n",
    "map_template = \"\"\"Ниже представлен список документов\n",
    "{docs}\n",
    "На основе этих документов определи, пожалуйста, основные темы \n",
    "Полезный ответ:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "reduce_template = \"\"\"Ниже представлен набор кратких изложений:\n",
    "{docs}\n",
    "Возьми их и сведи в окончательное, объединенное краткое изложение основных тем. \n",
    "Полезный ответ:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "\n",
    "# Run chain\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"docs\"\n",
    ")\n",
    "\n",
    "# Combines and iteratively reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=8000,\n",
    ")\n",
    "\n",
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=2000, chunk_overlap=0\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "map_reduce_chain.run(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Поиск терминов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'WAN': 'WAN - Глобальная компьютерная сеть (англ: Wide Area Network).',\n",
       " 'LAN': 'LAN - Local Area Network',\n",
       " 'MAC': 'Расшифровка аббревиатуры \"MAC\" в данном тексте - \"Media Access Control\" (надзор за доступом к среде).'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import AnalyzeDocumentChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "note_path = \"./notes/test_note.md\"\n",
    "\n",
    "loader = TextLoader(note_path, \"utf-8\")\n",
    "docs = loader.load()\n",
    "\n",
    "prompt_template = \"\"\"Твоя задача найти в тексте термины без определений.\n",
    "Игнорируй обычные слова. Ищи только термины, определения и сокращения, которым не хватает расшифровки.\n",
    "Если таких нет, ничего не возвращай.\n",
    "Текст: \"{context}\"\n",
    "Ответ:\"\"\"\n",
    "# prompt = PromptTemplate.from_template(prompt_template)\n",
    "# stuff_chain = create_stuff_documents_chain(llm, prompt)\n",
    "# stuff_chain.invoke({\"context\": docs})\n",
    "\n",
    "qa_chain = load_qa_chain(llm, chain_type=\"map_reduce\")\n",
    "qa_document_chain = AnalyzeDocumentChain(combine_docs_chain=qa_chain)\n",
    "abbreviations = qa_document_chain.run(\n",
    "    input_document=docs[0].page_content,\n",
    "    question=\"\"\"Найди в тексте сокращения и аббревиатуры. Не выбирай нормальные слова. Выведи их в формате [термин_1, термин_2,\n",
    "    термин_3, термин_4]. Не мотивируй свой ответ, не выводи другой текст.\"\"\",\n",
    ")\n",
    "\n",
    "abbreviations = list(set(abbreviations[abbreviations.find('['):abbreviations.find(']')].replace('\"', '').replace(\"'\", '').strip('][').split(', ')))\n",
    "\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "search = GoogleSerperAPIWrapper(serper_api_key=SERPER_API_KEY, gl='ru', hl='ru', k=5)\n",
    "\n",
    "search_results = {}\n",
    "for abb in abbreviations:\n",
    "    search_results[abb] = search.run(f'{abb} это')\n",
    "\n",
    "decodings = {}\n",
    "for abbreviation, search_result in search_results.items():\n",
    "    decoding = llm.invoke(f'''Найди расшифровку аббривиатуры \"{abbreviation}\" в тексте:\n",
    "                          \"{search_result}\". Выведи только расшифровку. Не выводи опредеоление и другой текст. Не объясняй свой ответ.''').content\n",
    "    decodings[abbreviation] = decoding\n",
    "\n",
    "decodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roadmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Введение в машинное обучение\n",
      "    - Основные понятия и определения машинного обучения\n",
      "    - Различия между контролируемым, неконтролируемым и полуконтролируемым обучением\n",
      "    - Примеры использования машинного обучения в различных областях\n",
      "\n",
      "2. Архитектуры машинного обучения\n",
      "    - Сверточные нейронные сети (CNN)\n",
      "        - Работа с изображениями и видео\n",
      "        - Фильтры, свертки, субдискретизация, пулинг\n",
      "    - Рекуррентные нейронные сети (RNN)\n",
      "        - Обработка последовательностей данных\n",
      "        - Использование в задачах обработки естественного языка и прогнозировании временных рядов\n",
      "    - Трансформеры\n",
      "        - Attention Mechanism\n",
      "        - Преимущества перед RNN\n",
      "\n",
      "3. Оценка моделей машинного обучения\n",
      "    - Валидация и тестирование моделей\n",
      "    - Выбор метрик для оценки производительности модели\n",
      "    - Кросс-валидация и время отклика\n",
      "\n",
      "4. Инференс и оптимизация моделей\n",
      "    - Введение в инференс\n",
      "    - Фреймворки для инференса (TensorFlow, PyTorch, MXNet и др.)\n",
      "    - Оптимизация моделей для инференса\n",
      "        - Quantization\n",
      "        - Pruning\n",
      "        - Distillation\n",
      "\n",
      "5. Развертывание моделей машинного обучения\n",
      "    - Выбор платформы для развертывания модели\n",
      "    - Контейнеризация и оркестрация\n",
      "    - Мониторинг и логирование\n",
      "    - Обеспечение безопасности и конфиденциальности данных\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Твоя задача - составить пользователю roadmap изучения указанной им темы/направления.\n",
    "            Посоветуй, что и в каком порядке ему следует изучить, чтобы полностью разобраться в теме.\n",
    "            Ответ пиши в формате списка. Составляй список в порядке возрастающей сложности. Следующие\n",
    "            шаги должны опираться на знания из предыдущих. Для каждого пункта добавь 2-3 предложения, \n",
    "            где описываются, что входит в эту тему. Ограничь список 5-7 пунктами. Структура ответа: \n",
    "            1. Что изучить\n",
    "                Состав программы\n",
    "            2. Что изучить\n",
    "                Состав программы\n",
    "            \n",
    "            Пример 1:\n",
    "            Тема - Data Science\n",
    "            Ответ:\n",
    "            1. Теория вероятностей и описательная статистика\n",
    "                категориальные и числовые данные, среднее значение, мода и медиана\n",
    "                стандартное отклонение и дисперсия, ковариация, корреляция, асимметрия\n",
    "                комбинаторика, события и их вероятности, классическая вероятность, условная вероятность\n",
    "                формулы Байеса, Пуассона и Бернулли, локальная и интегральная теоремы Лапласа\n",
    "                дискретные случайные величины, дискретные распределения (геометрическое, биномиальное, Пуассона)\n",
    "                непрерывные случайные величины, непрерывные распределения (равномерное, показательное, нормальное)\n",
    "                \n",
    "            2. Языки программирования Python​\n",
    "                типы данных: числа, строки, списки, множества, кортежи, циклы while и for, условия,\n",
    "                их комбинации, функции, область видимости, lambda, рекурсия, декораторы, генераторы,\n",
    "                вычислительная сложность, операции над структурами данных, стандартная библиотека,\n",
    "                работа с ошибками и исключениями, try-except, raise, assert, работа с файлами: чтение,\n",
    "                запись, сериализация, концепции ООП: полиморфизм, наследование, инкапсуляция\n",
    "                \n",
    "            3. Разные библиотеки, инструменты и техники Python для Data Science​\n",
    "                pandas, numpy, scipy, matplotlib, scikit-learn, tensorflow\n",
    "                \n",
    "            ​4. SQL и базы данных​\n",
    "                базовые концепции: таблицы, столбцы, строки и типы данных\n",
    "                создание простых SELECT-запросов\n",
    "                фильтрация с помощью WHERE и LIKE\n",
    "                агрегирующие функции: COUNT, SUM, AVG, MAX/MIN\n",
    "                группировка с помощью GROUP BY, HAVING\n",
    "                объединение таблиц через JOIN\n",
    "                CREATE TABLE для создания новых таблиц\n",
    "                ALTER TABLE, DROP TABLE для изменения и удаления\n",
    "                INSERT, UPDATE для добавления, изменения строк в таблице\n",
    "                DELETE для удаления строк\n",
    "                концепции базы данных (первичные и внешние ключи)\n",
    "                создание новой БД при помощи CREATE DATABASE\n",
    "\n",
    "            5. Машинное обучение​\n",
    "                классическое обучение (регрессия, классификация, кластеризация,\n",
    "                поиск правил, уменьшение размерности), ансамблевые методы ( стекинг,\n",
    "                беггинг, бустинг), обучение с подрекплением, нейросети и глубокое обучение\n",
    "\n",
    "            Пример 2:\n",
    "            Тема - алгоритмы\n",
    "            Ответ:\n",
    "            1. Списки и хеширование\n",
    "            2. Два указателя / Стек\n",
    "            3. Бинарный поиск / Скользящее окно / Связанные листы\n",
    "            4. Деревья\n",
    "            5. Нагруженные деревья / Бектрекинг\n",
    "            6. Куча / Графы / Одноразмерное динамическое программирование\n",
    "            7. Интервалы / Жадный алгоритм / Двухразмерное динамическое программирование\\n\n",
    "            манипуляции с битами / Продвинутые графы \n",
    "            \"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "learning_topic = 'ml system design'\n",
    "\n",
    "roadmap = chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=learning_topic\n",
    "            ),\n",
    "        ],\n",
    "    }\n",
    ").content\n",
    "\n",
    "print(roadmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
